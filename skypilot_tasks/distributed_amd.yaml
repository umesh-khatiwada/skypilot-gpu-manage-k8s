name: distributed-amd-training

num_nodes: 1 # Scale this up for multi-node

resources:
  cloud: kubernetes
  accelerators: MI300X:1
  cpus: 16
  memory: 128+
  image_id: docker:rocm/pytorch:latest

# Useful for DDP: enable communication between nodes
envs:
  NCCL_DEBUG: INFO
  RCCL_DEBUG: INFO # AMD specific debug flag

setup: |
  # Setup code here
  echo "Setting up distributed environment..."

run: |
  # Create a Python script for distributed training test
  cat > /tmp/test_dist.py << 'EOF'
  import torch
  import torch.distributed as dist
  
  dist.init_process_group('nccl')
  rank = dist.get_rank()
  world_size = dist.get_world_size()
  gpu_name = torch.cuda.get_device_name(0)
  print(f'Rank {rank} of {world_size} on {gpu_name} initialized')
  dist.destroy_process_group()
  EOF
  
  # Run with torchrun
  torchrun --nproc_per_node=1 --nnodes=${SKYPILOT_NUM_NODES:-1} --node_rank=${SKYPILOT_NODE_RANK:-0} --master_addr=${SKYPILOT_NODE_IPS:-127.0.0.1} --master_port=12345 /tmp/test_dist.py
