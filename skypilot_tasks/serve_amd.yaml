service:
  readiness_probe:
    path: /health
    initial_delay_seconds: 30
  replica_policy:
    min_replicas: 1
    max_replicas: 2
    target_qps_per_replica: 10

resources:
  cloud: kubernetes
  accelerators: MI300X:1
  cpus: 8
  memory: 64+
  image_id: docker:rocm/pytorch:latest
  ports:
    - 8000

setup: |
  pip install fastapi uvicorn torch

run: |
  echo "Starting mock LLM service on MI300X..."
  python3 <<EOF
  from fastapi import FastAPI
  import torch
  import uvicorn

  app = FastAPI()
  gpu_name = torch.cuda.get_device_name(0)

  @app.get("/health")
  def health():
      return {"status": "ok", "gpu": gpu_name}

  @app.post("/generate")
  def generate(prompt: str):
      # Simulate LLM generation
      return {"response": f"Generated text for: {prompt} using {gpu_name}"}

  if __name__ == "__main__":
      uvicorn.run(app, host="0.0.0.0", port=8000)
  EOF
